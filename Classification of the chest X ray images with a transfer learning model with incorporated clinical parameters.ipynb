{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f20491-155f-4662-8089-74812968f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5533319-84bd-44a5-8c2f-f3948c82b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXRDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None, clinical_cols=None, label_col=\"label\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.clinical_cols = clinical_cols\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Ensure clinical columns are numeric\n",
    "        for col in clinical_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")  # convert non-numeric → NaN\n",
    "        df = df.dropna(subset=clinical_cols + [label_col])     # drop rows missing clinical data or label\n",
    "\n",
    "        # Only keep images that exist in image_dir\n",
    "        df[\"image_path\"] = df[\"image_id\"].apply(lambda x: os.path.join(image_dir, x))\n",
    "        df = df[df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "        # Store cleaned dataframe\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ---- Load Image ----\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # ---- Clinical features ----\n",
    "        # Convert to numeric first, then fill NaN and cast\n",
    "        clinical_vals = pd.to_numeric(row[self.clinical_cols], errors=\"coerce\") \\\n",
    "                           .fillna(0).to_numpy(dtype=\"float32\")\n",
    "        clinical = torch.tensor(clinical_vals, dtype=torch.float)\n",
    "\n",
    "        # ---- Label ----\n",
    "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
    "\n",
    "        return img, clinical, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eda835-0b72-4504-9758-f0c2acaba113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2️⃣ Model: CNN + Clinical MLP + Fusion\n",
    "# -------------------------------\n",
    "class CXRClinicalFusionModel(nn.Module):\n",
    "    def __init__(self, num_tabular_features, num_classes):\n",
    "        super().__init__()\n",
    "        # CNN backbone\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  # Loads a pretrained ResNet-18.\n",
    "        self.cnn_encoder = nn.Sequential(*list(base.children())[:-1])   # Removes the final fully connected layer (fc) so we only keep the convolutional feature extractor.\n",
    "        # self.cnn_encoder now outputs a feature vector for each image.\n",
    "        self.cnn_dim = base.fc.in_features   # is equal to number of features produced by ResNet (usually 512).\n",
    "\n",
    "        # MLP for clinical features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.cnn_dim + 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, clinical):\n",
    "        img_feat = self.cnn_encoder(image).view(image.size(0), -1)\n",
    "        tab_feat = self.mlp(clinical)\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363aa751-929a-4613-8b04-a890aa290554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3️⃣ Image Transforms\n",
    "# -------------------------------\n",
    "train_tf = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_tf = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe5f167-590e-413d-baae-f6f187c21404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4️⃣ Dataset and Dataloader\n",
    "# -------------------------------\n",
    "clinical_cols = [\"age\", \"gender\", 'rotation', 'exposure']  # example columns\n",
    "label_col = \"label\"\n",
    "img_dir = r\"C:\\Users\\sureb\\my_data\\images\\primary_data\"\n",
    "img_dir1= r\"C:\\Users\\sureb\\my_data\\images\\secondary_data\"\n",
    "\n",
    "train_data = CXRDataset(\n",
    "    csv_path=  r\"C:\\Users\\sureb\\my_data\\primary_data.csv\",\n",
    "    image_dir=img_dir,\n",
    "    transform=train_tf,\n",
    "    clinical_cols=clinical_cols,\n",
    "    label_col=label_col\n",
    ")\n",
    "val_data = CXRDataset(\n",
    "    csv_path= r\"C:\\Users\\sureb\\my_data\\secondary_data.csv\",\n",
    "    image_dir=img_dir1,\n",
    "    transform=val_tf,\n",
    "    clinical_cols=clinical_cols,\n",
    "    label_col=label_col\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8e1e1c-3b8d-4d41-b82e-4282351d5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5️⃣ Training Setup\n",
    "# -------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CXRClinicalFusionModel(\n",
    "    num_tabular_features=len(clinical_cols),\n",
    "    num_classes=3\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96f6771-a97e-409e-90a0-1df8c6697af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for images, clinical, labels in loader:\n",
    "        images, clinical, labels = images.to(device), clinical.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()             # PyTorch accumulates gradients by default, so we must zero them every batch.\n",
    "        outputs = model(images, clinical)  # PyTorch internally calls: forward(images, clinical) method. \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()       # Computes gradients of the loss w.r.t. model parameters.\n",
    "        optimizer.step()         # Optimizer applies gradients to update the model’s parameters. This is the actual learning step.\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3143a9-c461-4ec5-8088-11a98ca10a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, clinical, labels in loader:\n",
    "            images, clinical, labels = images.to(device), clinical.to(device), labels.to(device)\n",
    "            outputs = model(images, clinical)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3a32aa-b2cb-4825-a31a-d116453c8ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.9497 | Train Acc: 0.5095 | Val Loss: 1.0992 | Val Acc: 0.4480\n",
      "Epoch 2/30 | Train Loss: 0.7395 | Train Acc: 0.7095 | Val Loss: 0.9796 | Val Acc: 0.4842\n",
      "Epoch 3/30 | Train Loss: 0.5242 | Train Acc: 0.8238 | Val Loss: 1.0410 | Val Acc: 0.4796\n",
      "Epoch 4/30 | Train Loss: 0.4020 | Train Acc: 0.8476 | Val Loss: 1.3450 | Val Acc: 0.4253\n",
      "Epoch 5/30 | Train Loss: 0.3705 | Train Acc: 0.9048 | Val Loss: 1.1616 | Val Acc: 0.4751\n",
      "Epoch 6/30 | Train Loss: 0.2635 | Train Acc: 0.9000 | Val Loss: 1.4017 | Val Acc: 0.3665\n",
      "Epoch 7/30 | Train Loss: 0.2066 | Train Acc: 0.9238 | Val Loss: 1.9573 | Val Acc: 0.2670\n",
      "Epoch 8/30 | Train Loss: 0.2197 | Train Acc: 0.9143 | Val Loss: 1.6093 | Val Acc: 0.2896\n",
      "Epoch 9/30 | Train Loss: 0.1455 | Train Acc: 0.9571 | Val Loss: 1.3904 | Val Acc: 0.3575\n",
      "Epoch 10/30 | Train Loss: 0.1801 | Train Acc: 0.9381 | Val Loss: 1.5583 | Val Acc: 0.4570\n",
      "Epoch 11/30 | Train Loss: 0.1962 | Train Acc: 0.9286 | Val Loss: 1.4623 | Val Acc: 0.4525\n",
      "Epoch 12/30 | Train Loss: 0.1104 | Train Acc: 0.9571 | Val Loss: 1.5631 | Val Acc: 0.4118\n",
      "Epoch 13/30 | Train Loss: 0.0901 | Train Acc: 0.9810 | Val Loss: 1.6902 | Val Acc: 0.4434\n",
      "Epoch 14/30 | Train Loss: 0.1407 | Train Acc: 0.9476 | Val Loss: 1.6216 | Val Acc: 0.4796\n",
      "Epoch 15/30 | Train Loss: 0.1535 | Train Acc: 0.9429 | Val Loss: 1.6298 | Val Acc: 0.4072\n",
      "Epoch 16/30 | Train Loss: 0.1463 | Train Acc: 0.9476 | Val Loss: 1.5403 | Val Acc: 0.4615\n",
      "Epoch 17/30 | Train Loss: 0.1142 | Train Acc: 0.9667 | Val Loss: 1.7635 | Val Acc: 0.3982\n",
      "Epoch 18/30 | Train Loss: 0.2262 | Train Acc: 0.9190 | Val Loss: 2.0927 | Val Acc: 0.3032\n",
      "Epoch 19/30 | Train Loss: 0.1395 | Train Acc: 0.9619 | Val Loss: 3.0532 | Val Acc: 0.2489\n",
      "Epoch 20/30 | Train Loss: 0.0945 | Train Acc: 0.9571 | Val Loss: 2.4684 | Val Acc: 0.3213\n",
      "Epoch 21/30 | Train Loss: 0.1239 | Train Acc: 0.9619 | Val Loss: 2.6904 | Val Acc: 0.3077\n",
      "Epoch 22/30 | Train Loss: 0.1772 | Train Acc: 0.9571 | Val Loss: 1.8828 | Val Acc: 0.4299\n",
      "Epoch 23/30 | Train Loss: 0.1037 | Train Acc: 0.9810 | Val Loss: 2.0570 | Val Acc: 0.4027\n",
      "Epoch 24/30 | Train Loss: 0.0774 | Train Acc: 0.9762 | Val Loss: 1.9951 | Val Acc: 0.3756\n",
      "Epoch 25/30 | Train Loss: 0.0710 | Train Acc: 0.9714 | Val Loss: 2.3564 | Val Acc: 0.3213\n",
      "Epoch 26/30 | Train Loss: 0.1283 | Train Acc: 0.9524 | Val Loss: 2.0829 | Val Acc: 0.3484\n",
      "Epoch 27/30 | Train Loss: 0.0897 | Train Acc: 0.9524 | Val Loss: 2.1790 | Val Acc: 0.2941\n",
      "Epoch 28/30 | Train Loss: 0.1279 | Train Acc: 0.9667 | Val Loss: 2.0190 | Val Acc: 0.3484\n",
      "Epoch 29/30 | Train Loss: 0.1546 | Train Acc: 0.9619 | Val Loss: 1.9327 | Val Acc: 0.3032\n",
      "Epoch 30/30 | Train Loss: 0.1371 | Train Acc: 0.9333 | Val Loss: 2.0337 | Val Acc: 0.3529\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a01e07-7d55-48ed-a121-97ff13ce9d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
